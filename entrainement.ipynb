{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle pré-entraîné de CamemBERT pour la classification de texte\n",
    "model_name = \"camembert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliser le pipeline de Hugging Face pour la classification de texte\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le fichier CSV et diviser les données en ensembles d'entraînement et de test\n",
    "def charger_donnees(filename):\n",
    "    # Lire le fichier CSV\n",
    "    df = pd.read_csv(filename, encoding=\"utf-8\")\n",
    "    \n",
    "    # Supprimer les lignes vides\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Appliquer BERT sur chaque avis pour prédire le sentiment\n",
    "    sentiments = []\n",
    "    for text in df[\"text\"]:\n",
    "        # Utilisation du modèle pré-entraîné CamemBERT pour prédire le sentiment\n",
    "        result = classifier(text)[0]  # Analyse de sentiment avec BERT\n",
    "        label = result['label']  # 'LABEL_0', 'LABEL_1', ou 'LABEL_2'\n",
    "\n",
    "        # Convertir en 0 = négatif, 1 = neutre, 2 = positif\n",
    "        if label == 'LABEL_0':  # Négatif\n",
    "            sentiments.append(0)\n",
    "        elif label == 'LABEL_1':  # Neutre\n",
    "            sentiments.append(1)\n",
    "        else:  # Positif\n",
    "            sentiments.append(2)\n",
    "\n",
    "    df[\"label\"] = sentiments  # Ajouter la colonne des labels\n",
    "\n",
    "    # Diviser les données (80% entraînement, 20% test)\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluer_modele(model, tokenizer, test_df):\n",
    "    inputs = tokenizer(list(test_df[\"text\"]), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    labels = torch.tensor(test_df[\"label\"].values)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\")\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualiser_evaluation(model, tokenizer, test_df):\n",
    "    # Tokenisation\n",
    "    inputs = tokenizer(list(test_df[\"text\"]), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    labels = torch.tensor(test_df[\"label\"].values)\n",
    "\n",
    "    # Prédictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Négatif\", \"Neutre\", \"Positif\"], yticklabels=[\"Négatif\", \"Neutre\", \"Positif\"])\n",
    "    plt.xlabel(\"Prédictions\")\n",
    "    plt.ylabel(\"Réel\")\n",
    "    plt.title(\"Matrice de Confusion\")\n",
    "    plt.show()\n",
    "\n",
    "    # Affichage des métriques sous forme de barres\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\")\n",
    "\n",
    "    metrics = {\"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1-score\": f1}\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(metrics.keys(), metrics.values(), color=[\"blue\", \"green\", \"orange\", \"red\"])\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(\"Scores du Modèle\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
